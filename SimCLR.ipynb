{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mariyamuneeb/ssl_wordspotting/blob/main/SimCLR.ipynb)"
      ],
      "metadata": {
        "id": "WTo2r5AV3JkJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.simplefilter('ignore')\n",
        "import os\n",
        "!git clone https://github.com/mariyamuneeb/ssl_wordspotting\n",
        "os.chdir('/content/ssl_wordspotting')\n",
        "!pip install -qqq wandb\n",
        "\n",
        "from setup_wandb import wandb_login\n",
        "wandb_login()"
      ],
      "metadata": {
        "id": "Ax5lAgzX8VrQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from data_utils.utils import copy_iam_dataset_to_colab\n",
        "copy_iam_dataset_to_colab()"
      ],
      "metadata": {
        "id": "jTXKgqgsBT7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C20nZuTc55OE"
      },
      "outputs": [],
      "source": [
        "## Standard libraries\n",
        "import os\n",
        "from copy import deepcopy\n",
        "\n",
        "## Imports for plotting\n",
        "import matplotlib.pyplot as plt\n",
        "plt.set_cmap('cividis')\n",
        "%matplotlib inline\n",
        "from IPython.display import set_matplotlib_formats\n",
        "set_matplotlib_formats('svg', 'pdf') # For export\n",
        "import matplotlib\n",
        "matplotlib.rcParams['lines.linewidth'] = 2.0\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "\n",
        "## tqdm for loading bars\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "## PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as data\n",
        "import torch.optim as optim\n",
        "\n",
        "## Torchvision\n",
        "import torchvision\n",
        "from torchvision.datasets import STL10\n",
        "from torchvision import transforms\n",
        "\n",
        "# PyTorch Lightning\n",
        "try:\n",
        "    import pytorch_lightning as pl\n",
        "except ModuleNotFoundError: # Google Colab does not have PyTorch Lightning installed by default. Hence, we do it here if necessary\n",
        "    !pip install --quiet pytorch-lightning>=1.4\n",
        "    import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
        "\n",
        "# Import tensorboard\n",
        "%load_ext tensorboard\n",
        "\n",
        "# Path to the folder where the datasets are/should be downloaded (e.g. CIFAR10)\n",
        "DATASET_PATH = \"../data\"\n",
        "# Path to the folder where the pretrained models are saved\n",
        "CHECKPOINT_PATH = \"../saved_models/tutorial17\"\n",
        "# In this notebook, we use data loaders with heavier computational processing. It is recommended to use as many\n",
        "# workers as possible in a data loader, which corresponds to the number of CPU cores\n",
        "NUM_WORKERS = os.cpu_count()\n",
        "\n",
        "# Setting the seed\n",
        "pl.seed_everything(42)\n",
        "\n",
        "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(\"Device:\", device)\n",
        "print(\"Number of workers:\", NUM_WORKERS)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "axZVKcFg8U2Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-0UOTza55OJ"
      },
      "source": [
        "## Loading Pretrained Resnet 18 Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uSClUcJL55OK"
      },
      "outputs": [],
      "source": [
        "import urllib.request\n",
        "from urllib.error import HTTPError\n",
        "# Github URL where saved models are stored for this tutorial\n",
        "base_url = \"https://raw.githubusercontent.com/phlippe/saved_models/main/tutorial17/\"\n",
        "# Files to download\n",
        "pretrained_files = [\"SimCLR.ckpt\", \"ResNet.ckpt\",\n",
        "                    \"tensorboards/SimCLR/events.out.tfevents.SimCLR\",\n",
        "                    \"tensorboards/classification/ResNet/events.out.tfevents.ResNet\"]\n",
        "pretrained_files += [f\"LogisticRegression_{size}.ckpt\" for size in [10, 20, 50, 100, 200, 500]]\n",
        "# Create checkpoint path if it doesn't exist yet\n",
        "os.makedirs(CHECKPOINT_PATH, exist_ok=True)\n",
        "\n",
        "# For each file, check whether it already exists. If not, try downloading it.\n",
        "for file_name in pretrained_files:\n",
        "    file_path = os.path.join(CHECKPOINT_PATH, file_name)\n",
        "    if \"/\" in file_name:\n",
        "        os.makedirs(file_path.rsplit(\"/\",1)[0], exist_ok=True)\n",
        "    if not os.path.isfile(file_path):\n",
        "        file_url = base_url + file_name\n",
        "        print(f\"Downloading {file_url}...\")\n",
        "        try:\n",
        "            urllib.request.urlretrieve(file_url, file_path)\n",
        "        except HTTPError as e:\n",
        "            print(\"Something went wrong. Please try to download the file from the GDrive folder, or contact the author with the full output including the following error:\\n\", e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPO1o6Za55OM"
      },
      "source": [
        "## SimCLR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01-QUPOr55OM"
      },
      "source": [
        "### Loading Dataset & Data Augmentation for Contrastive Learning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lrnlNY_a55OO"
      },
      "outputs": [],
      "source": [
        "class ContrastiveTransformations(object):\n",
        "    \n",
        "    def __init__(self, base_transforms, n_views=2):\n",
        "        self.base_transforms = base_transforms\n",
        "        self.n_views = n_views\n",
        "        \n",
        "    def __call__(self, x):\n",
        "        return [self.base_transforms(x) for i in range(self.n_views)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WJGQmChC55OS"
      },
      "outputs": [],
      "source": [
        "resize_size = (128,128)\n",
        "contrast_transforms = transforms.Compose([                                        \n",
        "                                          transforms.RandomHorizontalFlip(),\n",
        "                                          transforms.RandomResizedCrop(size=resize_size),\n",
        "                                            # transforms.Resize(resize_size),\n",
        "                                          transforms.RandomApply([\n",
        "                                          transforms.ColorJitter(brightness=0.5, \n",
        "                                                                     contrast=0.5, \n",
        "                                                                     saturation=0.5, \n",
        "                                                                     hue=0.1)\n",
        "                                          ], p=0.8),\n",
        "                                          \n",
        "                                          transforms.GaussianBlur(kernel_size=9),\n",
        "                                          transforms.ToTensor(),\n",
        "                                          transforms.Normalize((0.5,), (0.5,))\n",
        "                                         ])\n",
        "\n",
        "\n",
        "test_transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                     transforms.Resize(resize_size),])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q8u3af4M55OW"
      },
      "outputs": [],
      "source": [
        "unlabeled_data = STL10(root=DATASET_PATH, split='unlabeled', download=True, \n",
        "                       transform=ContrastiveTransformations(contrast_transforms, n_views=2))\n",
        "train_data_contrast = STL10(root=DATASET_PATH, split='train', download=True, \n",
        "                            transform=ContrastiveTransformations(contrast_transforms, n_views=2))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from models.dataset import IAMDataset2,IAMDataset,IAMSubset\n",
        "from models.layers import VariationalEncoder,Decoder\n",
        "from experiment_utils.utils import train_epoch,test_epoch\n",
        "from PIL import Image\n",
        "from math import floor\n",
        "from statistics import mean"
      ],
      "metadata": {
        "id": "P1hWW9959n3l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PzG26shs9JX5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "5rqbtVJyvRAA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "iam_train_dataset = IAMDataset2('train')\n",
        "iam_test_dataset = IAMDataset2('test')"
      ],
      "metadata": {
        "id": "Mhmibu3d9ynZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iam_train_dataset.transform = ContrastiveTransformations(contrast_transforms, n_views=2)\n",
        "iam_test_dataset.transform =  ContrastiveTransformations(contrast_transforms, n_views=2)"
      ],
      "metadata": {
        "id": "fAjKN0-r2OkU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "subset_iam_train_dataset = IAMSubset().subset(iam_train_dataset,0.2)\n",
        "subset_iam_test_dataset = IAMSubset().subset(iam_test_dataset,0.2)"
      ],
      "metadata": {
        "id": "wRAy2gm_9rSM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# subset_iam_train_dataset.transform = ContrastiveTransformations(contrast_transforms, n_views=2)\n",
        "# subset_iam_test_dataset = ContrastiveTransformations(contrast_transforms, n_views=2)"
      ],
      "metadata": {
        "id": "zWJ4NK7O1naH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "subset_iam_train_dataset[1][1]"
      ],
      "metadata": {
        "id": "uJuo8ooUv-A3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "he3ZK9iz55OY"
      },
      "outputs": [],
      "source": [
        "# Visualize some examples\n",
        "# pl.seed_everything(42)\n",
        "NUM_IMAGES = 6\n",
        "imgs = [img for idx in range(NUM_IMAGES) for img in torch.unsqueeze(subset_iam_train_dataset[idx][1],dim=0)]\n",
        "img_grid = torchvision.utils.make_grid(imgs, nrow=6, normalize=True, pad_value=0.9)\n",
        "img_grid = img_grid.permute(1, 2, 0)\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.title('Augmented image examples of the IAM HW dataset')\n",
        "plt.imshow(img_grid)\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "imgs[0].shape"
      ],
      "metadata": {
        "id": "mCcPVFaA0eFP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "subset_iam_train_dataset[1][1].shape"
      ],
      "metadata": {
        "id": "enAifD94v--7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbuiwO4155Ob"
      },
      "source": [
        "We see the wide variety of our data augmentation, including randomly cropping, grayscaling, gaussian blur, and color distortion. Thus, it remains a challenging task for the model to match two, independently augmented patches of the same image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnVVibpS55Oc"
      },
      "source": [
        "### SimCLR implementation\n",
        "\n",
        "Using the data loader pipeline above, we can now implement SimCLR. At each iteration, we get for every image $x$ two differently augmented versions, which we refer to as $\\tilde{x}_i$ and $\\tilde{x}_j$. Both of these images are encoded into a one-dimensional feature vector, between which we want to maximize similarity which minimizes it to all other images in the batch. The encoder network is split into two parts: a base encoder network $f(\\cdot)$, and a projection head $g(\\cdot)$. The base network is usually a deep CNN as we have seen in e.g. [Tutorial 5](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial5/Inception_ResNet_DenseNet.html) before, and is responsible for extracting a representation vector from the augmented data examples. In our experiments, we will use the common ResNet-18 architecture as $f(\\cdot)$, and refer to the output as $f(\\tilde{x}_i)=h_i$. The projection head $g(\\cdot)$ maps the representation $h$ into a space where we apply the contrastive loss, i.e., compare similarities between vectors. It is often chosen to be a small MLP with non-linearities, and for simplicity, we follow the original SimCLR paper setup by defining it as a two-layer MLP with ReLU activation in the hidden layer. Note that in the follow-up paper, [SimCLRv2](https://arxiv.org/abs/2006.10029), the authors mention that larger/wider MLPs can boost the performance considerably. This is why we apply an MLP with four times larger hidden dimensions, but deeper MLPs showed to overfit on the given dataset. The general setup is visualized below (figure credit - [Ting Chen et al.](https://arxiv.org/abs/2006.10029)):\n",
        "\n",
        "<center width=\"100%\"><img src=\"https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial17/simclr_network_setup.svg?raw=1\" width=\"350px\"></center>\n",
        "\n",
        "After finishing the training with contrastive learning, we will remove the projection head $g(\\cdot)$, and use $f(\\cdot)$ as a pretrained feature extractor. The representations $z$ that come out of the projection head $g(\\cdot)$ have been shown to perform worse than those of the base network $f(\\cdot)$ when finetuning the network for a new task. This is likely because the representations $z$ are trained to become invariant to many features like the color that can be important for downstream tasks. Thus, $g(\\cdot)$ is only needed for the contrastive learning stage.\n",
        "\n",
        "Now that the architecture is described, let's take a closer look at how we train the model. As mentioned before, we want to maximize the similarity between the representations of the two augmented versions of the same image, i.e., $z_i$ and $z_j$ in the figure above, while minimizing it to all other examples in the batch. SimCLR thereby applies the InfoNCE loss, originally proposed by [Aaron van den Oord et al.](https://arxiv.org/abs/1807.03748) for contrastive learning. In short, the InfoNCE loss compares the similarity of $z_i$ and $z_j$ to the similarity of $z_i$ to any other representation in the batch by performing a softmax over the similarity values. The loss can be formally written as:\n",
        "\n",
        "$$\n",
        "\\ell_{i,j}=-\\log \\frac{\\exp(\\text{sim}(z_i,z_j)/\\tau)}{\\sum_{k=1}^{2N}\\mathbb{1}_{[k\\neq i]}\\exp(\\text{sim}(z_i,z_k)/\\tau)}=-\\text{sim}(z_i,z_j)/\\tau+\\log\\left[\\sum_{k=1}^{2N}\\mathbb{1}_{[k\\neq i]}\\exp(\\text{sim}(z_i,z_k)/\\tau)\\right]\n",
        "$$\n",
        "\n",
        "The function $\\text{sim}$ is a similarity metric, and the hyperparameter $\\tau$ is called temperature determining how peaked the distribution is. Since many similarity metrics are bounded, the temperature parameter allows us to balance the influence of many dissimilar image patches versus one similar patch. The similarity metric that is used in SimCLR is cosine similarity, as defined below:\n",
        "\n",
        "$$\n",
        "\\text{sim}(z_i,z_j) = \\frac{z_i^\\top \\cdot z_j}{||z_i||\\cdot||z_j||}\n",
        "$$\n",
        "\n",
        "The maximum cosine similarity possible is $1$, while the minimum is $-1$. In general, we will see that the features of two different images will converge to a cosine similarity around zero since the minimum, $-1$, would require $z_i$ and $z_j$ to be in the exact opposite direction in all feature dimensions, which does not allow for great flexibility.\n",
        "\n",
        "Finally, now that we have discussed all details, let's implement SimCLR below as a PyTorch Lightning module:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1lUujOnP55Oe"
      },
      "outputs": [],
      "source": [
        "class SimCLR(pl.LightningModule):\n",
        "    \n",
        "    def __init__(self, hidden_dim, lr, temperature, weight_decay, max_epochs=500):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        assert self.hparams.temperature > 0.0, 'The temperature must be a positive float!'\n",
        "        # Base model f(.)\n",
        "        self.convnet = torchvision.models.resnet18(num_classes=4*hidden_dim)  # Output of last linear layer\n",
        "        # The MLP for g(.) consists of Linear->ReLU->Linear \n",
        "        self.convnet.fc = nn.Sequential(\n",
        "            self.convnet.fc,  # Linear(ResNet output, 4*hidden_dim)\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(4*hidden_dim, hidden_dim)\n",
        "        )\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = optim.AdamW(self.parameters(), \n",
        "                                lr=self.hparams.lr, \n",
        "                                weight_decay=self.hparams.weight_decay)\n",
        "        lr_scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer,\n",
        "                                                            T_max=self.hparams.max_epochs,\n",
        "                                                            eta_min=self.hparams.lr/50)\n",
        "        return [optimizer], [lr_scheduler]\n",
        "        \n",
        "    def info_nce_loss(self, batch, mode='train'):\n",
        "        imgs, _ = batch\n",
        "        imgs = torch.cat(imgs, dim=0)\n",
        "        \n",
        "        # Encode all images\n",
        "        feats = self.convnet(imgs)\n",
        "        # Calculate cosine similarity\n",
        "        cos_sim = F.cosine_similarity(feats[:,None,:], feats[None,:,:], dim=-1)\n",
        "        # Mask out cosine similarity to itself\n",
        "        self_mask = torch.eye(cos_sim.shape[0], dtype=torch.bool, device=cos_sim.device)\n",
        "        cos_sim.masked_fill_(self_mask, -9e15)\n",
        "        # Find positive example -> batch_size//2 away from the original example\n",
        "        pos_mask = self_mask.roll(shifts=cos_sim.shape[0]//2, dims=0)\n",
        "        # InfoNCE loss\n",
        "        cos_sim = cos_sim / self.hparams.temperature\n",
        "        nll = -cos_sim[pos_mask] + torch.logsumexp(cos_sim, dim=-1)\n",
        "        nll = nll.mean()\n",
        "        \n",
        "        # Logging loss\n",
        "        self.log(mode+'_loss', nll)\n",
        "        # Get ranking position of positive example\n",
        "        comb_sim = torch.cat([cos_sim[pos_mask][:,None],  # First position positive example\n",
        "                              cos_sim.masked_fill(pos_mask, -9e15)], \n",
        "                             dim=-1)\n",
        "        sim_argsort = comb_sim.argsort(dim=-1, descending=True).argmin(dim=-1)\n",
        "        # Logging ranking metrics\n",
        "        self.log(mode+'_acc_top1', (sim_argsort == 0).float().mean())\n",
        "        self.log(mode+'_acc_top5', (sim_argsort < 5).float().mean())\n",
        "        self.log(mode+'_acc_mean_pos', 1+sim_argsort.float().mean())\n",
        "        \n",
        "        return nll\n",
        "        \n",
        "    def training_step(self, batch, batch_idx):\n",
        "        return self.info_nce_loss(batch, mode='train')\n",
        "        \n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        self.info_nce_loss(batch, mode='val')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BH3BGC_M55Og"
      },
      "source": [
        "Alternatively to performing the validation on the contrastive learning loss as well, we could also take a simple, small downstream task, and track the performance of the base network $f(\\cdot)$ on that. However, in this tutorial, we will restrict ourselves to the STL10 dataset where we use the task of image classification on STL10 as our test task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cByagHHS55Oh"
      },
      "source": [
        "### Training\n",
        "\n",
        "Now that we have implemented SimCLR and the data loading pipeline, we are ready to train the model. We will use the same training function setup as usual. For saving the best model checkpoint, we track the metric `val_acc_top5`, which describes how often the correct image patch is within the top-5 most similar examples in the batch. This is usually less noisy than the top-1 metric, making it a better metric to choose the best model from."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8waehX2h55Oh"
      },
      "outputs": [],
      "source": [
        "def train_simclr(batch_size, max_epochs=500, **kwargs):\n",
        "    trainer = pl.Trainer(default_root_dir=os.path.join(CHECKPOINT_PATH, 'SimCLR'),\n",
        "                         accelerator=\"gpu\" if str(device).startswith(\"cuda\") else \"cpu\",\n",
        "                         devices=1,\n",
        "                         max_epochs=max_epochs,\n",
        "                         callbacks=[ModelCheckpoint(save_weights_only=True, mode='max', monitor='val_acc_top5'),\n",
        "                                    LearningRateMonitor('epoch')])\n",
        "    trainer.logger._default_hp_metric = None # Optional logging argument that we don't need\n",
        "\n",
        "    # Check whether pretrained model exists. If yes, load it and skip training\n",
        "    pretrained_filename = os.path.join(CHECKPOINT_PATH, 'SimCLR.ckpt')\n",
        "    if os.path.isfile(pretrained_filename):\n",
        "        print(f'Found pretrained model at {pretrained_filename}, loading...')\n",
        "        model = SimCLR.load_from_checkpoint(pretrained_filename) # Automatically loads the model with the saved hyperparameters\n",
        "    else:\n",
        "        train_loader = data.DataLoader(unlabeled_data, batch_size=batch_size, shuffle=True, \n",
        "                                       drop_last=True, pin_memory=True, num_workers=NUM_WORKERS)\n",
        "        val_loader = data.DataLoader(train_data_contrast, batch_size=batch_size, shuffle=False, \n",
        "                                     drop_last=False, pin_memory=True, num_workers=NUM_WORKERS)\n",
        "        pl.seed_everything(42) # To be reproducable\n",
        "        model = SimCLR(max_epochs=max_epochs, **kwargs)\n",
        "        trainer.fit(model, train_loader, val_loader)\n",
        "        model = SimCLR.load_from_checkpoint(trainer.checkpoint_callback.best_model_path) # Load best checkpoint after training\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3X5quTQg55Oi"
      },
      "source": [
        "A common observation in contrastive learning is that the larger the batch size, the better the models perform. A larger batch size allows us to compare each image to more negative examples, leading to overall smoother loss gradients. However, in our case, we experienced that a batch size of 256 was sufficient to get good results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J8AuCME955Oj"
      },
      "outputs": [],
      "source": [
        "simclr_model = train_simclr(batch_size=256, \n",
        "                            hidden_dim=128, \n",
        "                            lr=5e-4, \n",
        "                            temperature=0.07, \n",
        "                            weight_decay=1e-4, \n",
        "                            max_epochs=500)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "anVTSutX55Ok"
      },
      "source": [
        "To get an intuition of how training with contrastive learning behaves, we can take a look at the TensorBoard below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6nQSnqJ55Ok"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir ../saved_models/tutorial17/tensorboards/SimCLR/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GqrB0McU55Ol"
      },
      "source": [
        "<center width=\"100%\"><img src=\"https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial17/tensorboard_simclr.png?raw=1\" width=\"1200px\"></center>\n",
        "\n",
        "One thing to note is that contrastive learning benefits a lot from long training. The shown plot above is from a training that took approx. 1 day on a NVIDIA TitanRTX. Training the model for even longer might reduce its loss further, but we did not experience any gains from it for the downstream task on image classification. In general, contrastive learning can also benefit from using larger models, if sufficient unlabeled data is available."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypN-sT-w55Ol"
      },
      "source": [
        "## Logistic Regression\n",
        "\n",
        "After we have trained our model via contrastive learning, we can deploy it on downstream tasks and see how well it performs with little data. A common setup, which also verifies whether the model has learned generalized representations, is to perform Logistic Regression on the features. In other words, we learn a single, linear layer that maps the representations to a class prediction. Since the base network $f(\\cdot)$ is not changed during the training process, the model can only perform well if the representations of $h$ describe all features that might be necessary for the task. Further, we do not have to worry too much about overfitting since we have very few parameters that are trained. Hence, we might expect that the model can perform well even with very little data.\n",
        "\n",
        "First, let's implement a simple Logistic Regression setup for which we assume that the images already have been encoded in their feature vectors. If very little data is available, it might be beneficial to dynamically encode the images during training so that we can also apply data augmentations. However, the way we implement it here is much more efficient and can be trained within a few seconds. Further, using data augmentations did not show any significant gain in this simple setup."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VffV8KPX55Om"
      },
      "outputs": [],
      "source": [
        "class LogisticRegression(pl.LightningModule):\n",
        "    \n",
        "    def __init__(self, feature_dim, num_classes, lr, weight_decay, max_epochs=100):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        # Mapping from representation h to classes\n",
        "        self.model = nn.Linear(feature_dim, num_classes)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = optim.AdamW(self.parameters(), \n",
        "                                lr=self.hparams.lr, \n",
        "                                weight_decay=self.hparams.weight_decay)\n",
        "        lr_scheduler = optim.lr_scheduler.MultiStepLR(optimizer, \n",
        "                                                      milestones=[int(self.hparams.max_epochs*0.6), \n",
        "                                                                  int(self.hparams.max_epochs*0.8)], \n",
        "                                                      gamma=0.1)\n",
        "        return [optimizer], [lr_scheduler]\n",
        "        \n",
        "    def _calculate_loss(self, batch, mode='train'):\n",
        "        feats, labels = batch\n",
        "        preds = self.model(feats)\n",
        "        loss = F.cross_entropy(preds, labels)\n",
        "        acc = (preds.argmax(dim=-1) == labels).float().mean()\n",
        "\n",
        "        self.log(mode + '_loss', loss)\n",
        "        self.log(mode + '_acc', acc)\n",
        "        return loss        \n",
        "        \n",
        "    def training_step(self, batch, batch_idx):\n",
        "        return self._calculate_loss(batch, mode='train')\n",
        "        \n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        self._calculate_loss(batch, mode='val')\n",
        "        \n",
        "    def test_step(self, batch, batch_idx):\n",
        "        self._calculate_loss(batch, mode='test')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AoQ3otao55On"
      },
      "source": [
        "The data we use is the training and test set of STL10. The training contains 500 images per class, while the test set has 800 images per class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D7_Pumn055Oo"
      },
      "outputs": [],
      "source": [
        "img_transforms = transforms.Compose([transforms.ToTensor(),\n",
        "                                     transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "train_img_data = STL10(root=DATASET_PATH, split='train', download=True,\n",
        "                       transform=img_transforms)\n",
        "test_img_data = STL10(root=DATASET_PATH, split='test', download=True,\n",
        "                      transform=img_transforms)\n",
        "\n",
        "print(\"Number of training examples:\", len(train_img_data))\n",
        "print(\"Number of test examples:\", len(test_img_data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5DtBiYD55Op"
      },
      "source": [
        "Next, we implement a small function to encode all images in our datasets. The output representations are then used as inputs to the Logistic Regression model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dipETlA955Op"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def prepare_data_features(model, dataset):\n",
        "    # Prepare model\n",
        "    network = deepcopy(model.convnet)\n",
        "    network.fc = nn.Identity()  # Removing projection head g(.)\n",
        "    network.eval()\n",
        "    network.to(device)\n",
        "    \n",
        "    # Encode all images\n",
        "    data_loader = data.DataLoader(dataset, batch_size=64, num_workers=NUM_WORKERS, shuffle=False, drop_last=False)\n",
        "    feats, labels = [], []\n",
        "    for batch_imgs, batch_labels in tqdm(data_loader):\n",
        "        batch_imgs = batch_imgs.to(device)\n",
        "        batch_feats = network(batch_imgs)\n",
        "        feats.append(batch_feats.detach().cpu())\n",
        "        labels.append(batch_labels)\n",
        "    \n",
        "    feats = torch.cat(feats, dim=0)\n",
        "    labels = torch.cat(labels, dim=0)\n",
        "    \n",
        "    # Sort images by labels\n",
        "    labels, idxs = labels.sort()\n",
        "    feats = feats[idxs]\n",
        "    \n",
        "    return data.TensorDataset(feats, labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRhkvc3Z55Oq"
      },
      "source": [
        "Let's apply the function to both training and test set below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G4loxBPr55Oq"
      },
      "outputs": [],
      "source": [
        "train_feats_simclr = prepare_data_features(simclr_model, train_img_data)\n",
        "test_feats_simclr = prepare_data_features(simclr_model, test_img_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrdJEYYf55Or"
      },
      "source": [
        "Finally, we can write a training function as usual. We evaluate the model on the test set every 10 epochs to allow early stopping, but the low frequency of the validation ensures that we do not overfit too much on the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KVhKPpu155Os"
      },
      "outputs": [],
      "source": [
        "def train_logreg(batch_size, train_feats_data, test_feats_data, model_suffix, max_epochs=100, **kwargs):\n",
        "    trainer = pl.Trainer(default_root_dir=os.path.join(CHECKPOINT_PATH, \"LogisticRegression\"),\n",
        "                         accelerator=\"gpu\" if str(device).startswith(\"cuda\") else \"cpu\",\n",
        "                         devices=1,\n",
        "                         max_epochs=max_epochs,\n",
        "                         callbacks=[ModelCheckpoint(save_weights_only=True, mode='max', monitor='val_acc'),\n",
        "                                    LearningRateMonitor(\"epoch\")],\n",
        "                         enable_progress_bar=False,\n",
        "                         check_val_every_n_epoch=10)\n",
        "    trainer.logger._default_hp_metric = None\n",
        "    \n",
        "    # Data loaders\n",
        "    train_loader = data.DataLoader(train_feats_data, batch_size=batch_size, shuffle=True, \n",
        "                                   drop_last=False, pin_memory=True, num_workers=0)\n",
        "    test_loader = data.DataLoader(test_feats_data, batch_size=batch_size, shuffle=False, \n",
        "                                  drop_last=False, pin_memory=True, num_workers=0)\n",
        "\n",
        "    # Check whether pretrained model exists. If yes, load it and skip training\n",
        "    pretrained_filename = os.path.join(CHECKPOINT_PATH, f\"LogisticRegression_{model_suffix}.ckpt\")\n",
        "    if os.path.isfile(pretrained_filename):\n",
        "        print(f\"Found pretrained model at {pretrained_filename}, loading...\")\n",
        "        model = LogisticRegression.load_from_checkpoint(pretrained_filename)\n",
        "    else:\n",
        "        pl.seed_everything(42)  # To be reproducable\n",
        "        model = LogisticRegression(**kwargs)\n",
        "        trainer.fit(model, train_loader, test_loader)\n",
        "        model = LogisticRegression.load_from_checkpoint(trainer.checkpoint_callback.best_model_path)\n",
        "\n",
        "    # Test best model on train and validation set\n",
        "    train_result = trainer.test(model, train_loader, verbose=False)\n",
        "    test_result = trainer.test(model, test_loader, verbose=False)\n",
        "    result = {\"train\": train_result[0][\"test_acc\"], \"test\": test_result[0][\"test_acc\"]}\n",
        "        \n",
        "    return model, result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTUlXnBH55Os"
      },
      "source": [
        "Despite the training dataset of STL10 already only having 500 labeled images per class, we will perform experiments with even smaller datasets. Specifically, we train a Logistic Regression model for datasets with only 10, 20, 50, 100, 200, and all 500 examples per class. This gives us an intuition on how well the representations learned by contrastive learning can be transfered to a image recognition task like this classification. First, let's define a function to create the intended sub-datasets from the full training set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hbcMKwQG55Ot"
      },
      "outputs": [],
      "source": [
        "def get_smaller_dataset(original_dataset, num_imgs_per_label):\n",
        "    new_dataset = data.TensorDataset(\n",
        "        *[t.unflatten(0, (10, -1))[:,:num_imgs_per_label].flatten(0, 1) for t in original_dataset.tensors]\n",
        "    )\n",
        "    return new_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJQzyzC955Ou"
      },
      "source": [
        "Next, let's run all models. Despite us training 6 models, this cell could be run within a minute or two without the pretrained models. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "GKJaKgUe55Ou"
      },
      "outputs": [],
      "source": [
        "results = {}\n",
        "for num_imgs_per_label in [10, 20, 50, 100, 200, 500]:\n",
        "    sub_train_set = get_smaller_dataset(train_feats_simclr, num_imgs_per_label)\n",
        "    _, small_set_results = train_logreg(batch_size=64,\n",
        "                                        train_feats_data=sub_train_set,\n",
        "                                        test_feats_data=test_feats_simclr,\n",
        "                                        model_suffix=num_imgs_per_label,\n",
        "                                        feature_dim=train_feats_simclr.tensors[0].shape[1],\n",
        "                                        num_classes=10,\n",
        "                                        lr=1e-3,\n",
        "                                        weight_decay=1e-3)\n",
        "    results[num_imgs_per_label] = small_set_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gTjbTGv55Ov"
      },
      "source": [
        "Finally, let's plot the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uy0Czg4n55Ox"
      },
      "outputs": [],
      "source": [
        "dataset_sizes = sorted([k for k in results])\n",
        "test_scores = [results[k][\"test\"] for k in dataset_sizes]\n",
        "\n",
        "fig = plt.figure(figsize=(6,4))\n",
        "plt.plot(dataset_sizes, test_scores, '--', color=\"#000\", marker=\"*\", markeredgecolor=\"#000\", markerfacecolor=\"y\", markersize=16)\n",
        "plt.xscale(\"log\")\n",
        "plt.xticks(dataset_sizes, labels=dataset_sizes)\n",
        "plt.title(\"STL10 classification over dataset size\", fontsize=14)\n",
        "plt.xlabel(\"Number of images per class\")\n",
        "plt.ylabel(\"Test accuracy\")\n",
        "plt.minorticks_off()\n",
        "plt.show()\n",
        "\n",
        "for k, score in zip(dataset_sizes, test_scores):\n",
        "    print(f'Test accuracy for {k:3d} images per label: {100*score:4.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oe5Ywt4L55Oy"
      },
      "source": [
        "As one would expect, the classification performance improves the more data we have. However, with only 10 images per class, we can already classify more than 60% of the images correctly. This is quite impressive, considering that the images are also higher dimensional than e.g. CIFAR10. With the full dataset, we achieve an accuracy of 81%. The increase between 50 to 500 images per class might suggest a linear increase in performance with an exponentially larger dataset. However, with even more data, we could also finetune $f(\\cdot)$ in the training process, allowing for the representations to adapt more to the specific classification task given.\n",
        "\n",
        "To set the results above into perspective, we will train the base network, a ResNet-18, on the classification task from scratch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dWTOPGT55Oz"
      },
      "source": [
        "## Baseline\n",
        "\n",
        "As a baseline to our results above, we will train a standard ResNet-18 with random initialization on the labeled training set of STL10. The results will give us an indication of the advantages that contrastive learning on unlabeled data has compared to using only supervised training. The implementation of the model is straightforward since the ResNet architecture is provided in the torchvision library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-zg-gpCa55Oz"
      },
      "outputs": [],
      "source": [
        "class ResNet(pl.LightningModule):\n",
        "\n",
        "    def __init__(self, num_classes, lr, weight_decay, max_epochs=100):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.model = torchvision.models.resnet18(num_classes=num_classes)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = optim.AdamW(self.parameters(), \n",
        "                                lr=self.hparams.lr, \n",
        "                                weight_decay=self.hparams.weight_decay)\n",
        "        lr_scheduler = optim.lr_scheduler.MultiStepLR(optimizer, \n",
        "                                                      milestones=[int(self.hparams.max_epochs*0.7), \n",
        "                                                                  int(self.hparams.max_epochs*0.9)], \n",
        "                                                      gamma=0.1)\n",
        "        return [optimizer], [lr_scheduler]\n",
        "\n",
        "    def _calculate_loss(self, batch, mode='train'):\n",
        "        imgs, labels = batch\n",
        "        preds = self.model(imgs)\n",
        "        loss = F.cross_entropy(preds, labels)\n",
        "        acc = (preds.argmax(dim=-1) == labels).float().mean()\n",
        "\n",
        "        self.log(mode + '_loss', loss)\n",
        "        self.log(mode + '_acc', acc)\n",
        "        return loss\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        return self._calculate_loss(batch, mode='train')\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        self._calculate_loss(batch, mode='val')\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        self._calculate_loss(batch, mode='test')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18SCIdUW55O0"
      },
      "source": [
        "It is clear that the ResNet easily overfits on the training data since its parameter count is more than 1000 times larger than the dataset size. To make the comparison to the contrastive learning models fair, we apply data augmentations similar to the ones we used before: horizontal flip, crop-and-resize, grayscale, and gaussian blur. Color distortions as before are not used because the color distribution of an image showed to be an important feature for the classification. Hence, we observed no noticeable performance gains when adding color distortions to the set of augmentations. Similarly, we restrict the resizing operation before cropping to the max. 125% of its original resolution, instead of 1250% as done in SimCLR. This is because, for classification, the model needs to recognize the full object, while in contrastive learning, we only want to check whether two patches belong to the same image/object. Hence, the chosen augmentations below are overall weaker than in the contrastive learning case."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d_JQDg6r55O1"
      },
      "outputs": [],
      "source": [
        "train_transforms = transforms.Compose([transforms.RandomHorizontalFlip(),\n",
        "                                       transforms.RandomResizedCrop(size=96, scale=(0.8, 1.0)),\n",
        "                                       transforms.RandomGrayscale(p=0.2),\n",
        "                                       transforms.GaussianBlur(kernel_size=9, sigma=(0.1, 0.5)),\n",
        "                                       transforms.ToTensor(),\n",
        "                                       transforms.Normalize((0.5,), (0.5,))\n",
        "                                       ])\n",
        "\n",
        "train_img_aug_data = STL10(root=DATASET_PATH, split='train', download=True,\n",
        "                           transform=train_transforms)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2EWKFREZ55O3"
      },
      "source": [
        "The training function for the ResNet is almost identical to the Logistic Regression setup. Note that we allow the ResNet to perform validation every 2 epochs to also check whether the model overfits strongly in the first iterations or not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ylIYSaE55O3"
      },
      "outputs": [],
      "source": [
        "def train_resnet(batch_size, max_epochs=100, **kwargs):\n",
        "    trainer = pl.Trainer(default_root_dir=os.path.join(CHECKPOINT_PATH, \"ResNet\"),\n",
        "                         accelerator=\"gpu\" if str(device).startswith(\"cuda\") else \"cpu\",\n",
        "                         devices=1,\n",
        "                         max_epochs=max_epochs,\n",
        "                         callbacks=[ModelCheckpoint(save_weights_only=True, mode=\"max\", monitor=\"val_acc\"),\n",
        "                                    LearningRateMonitor(\"epoch\")],\n",
        "                         check_val_every_n_epoch=2)\n",
        "    trainer.logger._default_hp_metric = None\n",
        "    \n",
        "    # Data loaders\n",
        "    train_loader = data.DataLoader(train_img_aug_data, batch_size=batch_size, shuffle=True, \n",
        "                                   drop_last=True, pin_memory=True, num_workers=NUM_WORKERS)\n",
        "    test_loader = data.DataLoader(test_img_data, batch_size=batch_size, shuffle=False, \n",
        "                                  drop_last=False, pin_memory=True, num_workers=NUM_WORKERS)\n",
        "\n",
        "    # Check whether pretrained model exists. If yes, load it and skip training\n",
        "    pretrained_filename = os.path.join(CHECKPOINT_PATH, \"ResNet.ckpt\")\n",
        "    if os.path.isfile(pretrained_filename):\n",
        "        print(\"Found pretrained model at %s, loading...\" % pretrained_filename)\n",
        "        model = ResNet.load_from_checkpoint(pretrained_filename)\n",
        "    else:\n",
        "        pl.seed_everything(42) # To be reproducable\n",
        "        model = ResNet(**kwargs)\n",
        "        trainer.fit(model, train_loader, test_loader)\n",
        "        model = ResNet.load_from_checkpoint(trainer.checkpoint_callback.best_model_path)\n",
        "\n",
        "    # Test best model on validation set\n",
        "    train_result = trainer.test(model, train_loader, verbose=False)\n",
        "    val_result = trainer.test(model, test_loader, verbose=False)\n",
        "    result = {\"train\": train_result[0][\"test_acc\"], \"test\": val_result[0][\"test_acc\"]}\n",
        "        \n",
        "    return model, result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPcStcVt55O5"
      },
      "source": [
        "Finally, let's train the model and check its results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HI4q2zIR55O6"
      },
      "outputs": [],
      "source": [
        "resnet_model, resnet_result = train_resnet(batch_size=64,\n",
        "                                           num_classes=10,\n",
        "                                           lr=1e-3,\n",
        "                                           weight_decay=2e-4,\n",
        "                                           max_epochs=100)\n",
        "print(f\"Accuracy on training set: {100*resnet_result['train']:4.2f}%\")\n",
        "print(f\"Accuracy on test set: {100*resnet_result['test']:4.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xc6c36RZ55O7"
      },
      "source": [
        "The ResNet trained from scratch achieves 73.31% on the test set. This is almost 8% less than the contrastive learning model, and even slightly less than SimCLR achieves with 1/10 of the data. This shows that self-supervised, contrastive learning provides considerable performance gains by leveraging large amounts of unlabeled data when little labeled data is available."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Six3Kwsm55O8"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "In this tutorial, we have discussed self-supervised contrastive learning and implemented SimCLR as an example method. We have applied it to the STL10 dataset and showed that it can learn generalizable representations that we can use to train simple classification models. With 500 images per label, it achieved an 8% higher accuracy than a similar model solely trained from supervision and performs on par with it when only using a tenth of the labeled data. Our experimental results are limited to a single dataset, but recent works such as [Ting Chen et al.](https://arxiv.org/abs/2006.10029) showed similar trends for larger datasets like ImageNet. Besides the discussed hyperparameters, the size of the model seems to be important in contrastive learning as well. If a lot of unlabeled data is available, larger models can achieve much stronger results and come close to their supervised baselines. Further, there are also approaches for combining contrastive and supervised learning, leading to performance gains beyond supervision (see [Khosla et al.](https://arxiv.org/abs/2004.11362)). Moreover, contrastive learning is not the only approach to self-supervised learning that has come up in the last two years and showed great results. Other methods include distillation-based methods like [BYOL](https://arxiv.org/abs/2006.07733) and redundancy reduction techniques like [Barlow Twins](https://arxiv.org/abs/2103.03230). There is a lot more to explore in the self-supervised domain, and more, impressive steps ahead are to be expected.\n",
        "\n",
        "### References\n",
        "\n",
        "[1] Chen, T., Kornblith, S., Norouzi, M., and Hinton, G. (2020). A simple framework for contrastive learning of visual representations. In International Conference on Machine Learning (ICML 2020). PMLR. ([link](https://arxiv.org/abs/2002.05709))\n",
        "\n",
        "[2] Chen, T., Kornblith, S., Swersky, K., Norouzi, M., and Hinton, G. (2020). Big self-supervised models are strong semi-supervised learners. NeurIPS 2021 ([link](https://arxiv.org/abs/2006.10029)).\n",
        "\n",
        "[3] Oord, A. V. D., Li, Y., and Vinyals, O. (2018). Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748. ([link](https://arxiv.org/abs/1807.03748))\n",
        "\n",
        "[4] Grill, J.B., Strub, F., Altch, F., Tallec, C., Richemond, P.H., Buchatskaya, E., Doersch, C., Pires, B.A., Guo, Z.D., Azar, M.G. and Piot, B. (2020). Bootstrap your own latent: A new approach to self-supervised learning. NeurIPS 2020 ([link](https://arxiv.org/abs/2006.07733))\n",
        "\n",
        "[5] Khosla, P., Teterwak, P., Wang, C., Sarna, A., Tian, Y., Isola, P., Maschinot, A., Liu, C. and Krishnan, D. (2020). Supervised contrastive learning. NeurIPS 2020 ([link](https://arxiv.org/abs/2004.11362))\n",
        "\n",
        "[6] Zbontar, J., Jing, L., Misra, I., LeCun, Y. and Deny, S. (2021). Barlow twins: Self-supervised learning via redundancy reduction. In International Conference on Machine Learning (ICML 2021). ([link](https://arxiv.org/abs/2103.03230))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9eDeTDf55O9"
      },
      "source": [
        "---\n",
        "\n",
        "[![Star our repository](https://img.shields.io/static/v1.svg?logo=star&label=&message=Star%20Our%20Repository&color=yellow)](https://github.com/phlippe/uvadlc_notebooks/)  If you found this tutorial helpful, consider -ing our repository.    \n",
        "[![Ask questions](https://img.shields.io/static/v1.svg?logo=star&label=&message=Ask%20Questions&color=9cf)](https://github.com/phlippe/uvadlc_notebooks/issues)  For any questions, typos, or bugs that you found, please raise an issue on GitHub. \n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start = 1100\n",
        "end = 1110\n",
        "for i in range(start,end+1):\n",
        "    print(i)\n",
        "    print('')"
      ],
      "metadata": {
        "id": "Xa5C5HT0uU5i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rW4Le5DPuaOn"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}